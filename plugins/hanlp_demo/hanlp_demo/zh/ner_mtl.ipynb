{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WfGpInivS0fG"
      },
      "source": [
        "<h2 align=\"center\">点击下列图标在线运行HanLP</h2>\n",
        "<div align=\"center\">\n",
        "\t<a href=\"https://colab.research.google.com/github/hankcs/HanLP/blob/doc-zh/plugins/hanlp_demo/hanlp_demo/zh/ner_mtl.ipynb\" target=\"_blank\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
        "\t<a href=\"https://mybinder.org/v2/gh/hankcs/HanLP/doc-zh?filepath=plugins%2Fhanlp_demo%2Fhanlp_demo%2Fzh%2Fner_mtl.ipynb\" target=\"_blank\"><img src=\"https://mybinder.org/badge_logo.svg\" alt=\"Open In Binder\"/></a>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IYwV-UkNNzFp"
      },
      "source": [
        "## 安装"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Uf_u7ddMhUt",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "无论是Windows、Linux还是macOS，HanLP的安装只需一句话搞定："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "pp-1KqEOOJ4t",
        "pycharm": {
          "name": "#%%\n"
        },
        "outputId": "dd4dea48-289f-40ae-96bf-a32c1674fab5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting hanlp\n",
            "  Downloading hanlp-2.1.0b62-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting hanlp-common>=0.0.20 (from hanlp)\n",
            "  Downloading hanlp_common-0.0.20.tar.gz (28 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting hanlp-downloader (from hanlp)\n",
            "  Downloading hanlp_downloader-0.0.25.tar.gz (13 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting hanlp-trie>=0.0.4 (from hanlp)\n",
            "  Downloading hanlp_trie-0.0.5.tar.gz (6.7 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pynvml (from hanlp)\n",
            "  Downloading pynvml-11.5.3-py3-none-any.whl.metadata (8.8 kB)\n",
            "Requirement already satisfied: sentencepiece>=0.1.91 in /usr/local/lib/python3.10/dist-packages (from hanlp) (0.2.0)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.10/dist-packages (from hanlp) (2.5.0)\n",
            "Collecting toposort==1.5 (from hanlp)\n",
            "  Downloading toposort-1.5-py2.py3-none-any.whl.metadata (5.7 kB)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from hanlp) (2.5.0+cu121)\n",
            "Requirement already satisfied: transformers>=4.1.1 in /usr/local/lib/python3.10/dist-packages (from hanlp) (4.44.2)\n",
            "Collecting phrasetree>=0.0.9 (from hanlp-common>=0.0.20->hanlp)\n",
            "  Downloading phrasetree-0.0.9.tar.gz (42 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->hanlp) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->hanlp) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->hanlp) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->hanlp) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->hanlp) (2024.6.1)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->hanlp) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.6.0->hanlp) (1.3.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.1.1->hanlp) (0.24.7)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.1.1->hanlp) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.1.1->hanlp) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.1.1->hanlp) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.1.1->hanlp) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers>=4.1.1->hanlp) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.1.1->hanlp) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.1.1->hanlp) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.1.1->hanlp) (4.66.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.6.0->hanlp) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.1.1->hanlp) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.1.1->hanlp) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.1.1->hanlp) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.1.1->hanlp) (2024.8.30)\n",
            "Downloading hanlp-2.1.0b62-py3-none-any.whl (651 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m651.8/651.8 kB\u001b[0m \u001b[31m17.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading toposort-1.5-py2.py3-none-any.whl (7.6 kB)\n",
            "Downloading pynvml-11.5.3-py3-none-any.whl (53 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.1/53.1 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: hanlp-common, hanlp-trie, hanlp-downloader, phrasetree\n",
            "  Building wheel for hanlp-common (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for hanlp-common: filename=hanlp_common-0.0.20-py3-none-any.whl size=30634 sha256=9d1535260f2ccd276f98f5c5a1a5274efcf1d7c6de974ffb7c36e1d9e5c860c4\n",
            "  Stored in directory: /root/.cache/pip/wheels/06/2c/84/7bdb23dafe011f728e07335a5e7687c6ad45576115964b21a6\n",
            "  Building wheel for hanlp-trie (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for hanlp-trie: filename=hanlp_trie-0.0.5-py3-none-any.whl size=6816 sha256=ce842182b25f6fe01466bd91fbf14cb0b6451a49f251dded3cb300f5fdeb480b\n",
            "  Stored in directory: /root/.cache/pip/wheels/4b/ff/a9/6883d860cbed0247ce316c39757ff61c1f5bfd3556abfbefa0\n",
            "  Building wheel for hanlp-downloader (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for hanlp-downloader: filename=hanlp_downloader-0.0.25-py3-none-any.whl size=13747 sha256=d7b81a50fed2a3f241941135e8100f5d0e0e2728ac8d94726df3fa5407839b6c\n",
            "  Stored in directory: /root/.cache/pip/wheels/71/2b/67/65892ee65ea68b4c0936b9a8f2ea73665c3389fec08746b949\n",
            "  Building wheel for phrasetree (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for phrasetree: filename=phrasetree-0.0.9-py3-none-any.whl size=44218 sha256=469cfc0a9b0434e36fcb654dd584dc8f6ba7fdf90c446df0919f0a81a40d0a64\n",
            "  Stored in directory: /root/.cache/pip/wheels/61/df/09/e14b1cc4c92a1ed7e1d52647fde6f3e1169ea9cf2df78f1936\n",
            "Successfully built hanlp-common hanlp-trie hanlp-downloader phrasetree\n",
            "Installing collected packages: toposort, phrasetree, pynvml, hanlp-common, hanlp-trie, hanlp-downloader, hanlp\n",
            "Successfully installed hanlp-2.1.0b62 hanlp-common-0.0.20 hanlp-downloader-0.0.25 hanlp-trie-0.0.5 phrasetree-0.0.9 pynvml-11.5.3 toposort-1.5\n"
          ]
        }
      ],
      "source": [
        "!pip install hanlp -U"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0tmKBu7sNAXX",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "## 加载模型\n",
        "HanLP的工作流程是先加载模型，模型的标示符存储在`hanlp.pretrained`这个包中，按照NLP任务归类。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EmZDmLn9aGxG",
        "outputId": "bc5a949d-c8ec-494f-a60d-835f04b5a629",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'OPEN_TOK_POS_NER_SRL_DEP_SDP_CON_ELECTRA_SMALL_ZH': 'https://file.hankcs.com/hanlp/mtl/open_tok_pos_ner_srl_dep_sdp_con_electra_small_20201223_035557.zip',\n",
              " 'OPEN_TOK_POS_NER_SRL_DEP_SDP_CON_ELECTRA_BASE_ZH': 'https://file.hankcs.com/hanlp/mtl/open_tok_pos_ner_srl_dep_sdp_con_electra_base_20201223_201906.zip',\n",
              " 'CLOSE_TOK_POS_NER_SRL_DEP_SDP_CON_ELECTRA_SMALL_ZH': 'https://file.hankcs.com/hanlp/mtl/close_tok_pos_ner_srl_dep_sdp_con_electra_small_20210111_124159.zip',\n",
              " 'CLOSE_TOK_POS_NER_SRL_UDEP_SDP_CON_ELECTRA_SMALL_ZH': 'https://file.hankcs.com/hanlp/mtl/close_tok_pos_ner_srl_dep_sdp_con_electra_small_20220626_175100.zip',\n",
              " 'CLOSE_TOK_POS_NER_SRL_DEP_SDP_CON_ELECTRA_BASE_ZH': 'https://file.hankcs.com/hanlp/mtl/close_tok_pos_ner_srl_dep_sdp_con_electra_base_20210111_124519.zip',\n",
              " 'CLOSE_TOK_POS_NER_SRL_DEP_SDP_CON_ERNIE_GRAM_ZH': 'https://file.hankcs.com/hanlp/mtl/close_tok_pos_ner_srl_dep_sdp_con_ernie_gram_base_aug_20210904_145403.zip',\n",
              " 'UD_ONTONOTES_TOK_POS_LEM_FEA_NER_SRL_DEP_SDP_CON_MMINILMV2L6': 'https://file.hankcs.com/hanlp/mtl/ud_ontonotes_tok_pos_lem_fea_ner_srl_dep_sdp_con_mMiniLMv2L6_no_space_20220731_161526.zip',\n",
              " 'UD_ONTONOTES_TOK_POS_LEM_FEA_NER_SRL_DEP_SDP_CON_MMINILMV2L12': 'https://file.hankcs.com/hanlp/mtl/ud_ontonotes_tok_pos_lem_fea_ner_srl_dep_sdp_con_mMiniLMv2L12_no_space_20220807_133143.zip',\n",
              " 'UD_ONTONOTES_TOK_POS_LEM_FEA_NER_SRL_DEP_SDP_CON_XLMR_BASE': 'https://file.hankcs.com/hanlp/mtl/ud_ontonotes_tok_pos_lem_fea_ner_srl_dep_sdp_con_xlm_base_20220608_003435.zip',\n",
              " 'NPCMJ_UD_KYOTO_TOK_POS_CON_BERT_BASE_CHAR_JA': 'https://file.hankcs.com/hanlp/mtl/npcmj_ud_kyoto_tok_pos_ner_dep_con_srl_bert_base_char_ja_20210914_133742.zip'}"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "import hanlp\n",
        "hanlp.pretrained.mtl.ALL # MTL多任务，具体任务见模型名称，语种见名称最后一个字段或相应语料库"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w0lm87NUsMwW"
      },
      "source": [
        "调用`hanlp.load`进行加载，模型会自动下载到本地缓存。自然语言处理分为许多任务，分词只是最初级的一个。与其每个任务单独创建一个模型，不如利用HanLP的联合模型一次性完成多个任务："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "6Evnxsa0sMwW",
        "pycharm": {
          "name": "#%%\n"
        },
        "outputId": "567410f2-63db-4d56-8355-3d4e2b9aa462",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading https://file.hankcs.com/hanlp/mtl/close_tok_pos_ner_srl_dep_sdp_con_electra_base_20210111_124519.zip to /root/.hanlp/mtl/close_tok_pos_ner_srl_dep_sdp_con_electra_base_20210111_124519.zip\n",
            "100% 467.9 MiB 343.3 KiB/s ETA:  0 s [=========================================]\n",
            "Decompressing /root/.hanlp/mtl/close_tok_pos_ner_srl_dep_sdp_con_electra_base_20210111_124519.zip to /root/.hanlp/mtl\n",
            "Downloading https://file.hankcs.com/hanlp/transformers/electra_zh_base_20210706_125233.zip to /root/.hanlp/transformers/electra_zh_base_20210706_125233.zip\n",
            "100%  41.2 KiB  41.2 KiB/s ETA:  0 s [=========================================]\n",
            "Decompressing /root/.hanlp/transformers/electra_zh_base_20210706_125233.zip to /root/.hanlp/transformers\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Downloading https://file.hankcs.com/corpus/char_table.json.zip to /root/.hanlp/thirdparty/file.hankcs.com/corpus/char_table.json.zip\n",
            "100%  19.4 KiB  19.4 KiB/s ETA:  0 s [=========================================]\n",
            "Decompressing /root/.hanlp/thirdparty/file.hankcs.com/corpus/char_table.json.zip to /root/.hanlp/thirdparty/file.hankcs.com/corpus\n"
          ]
        }
      ],
      "source": [
        "HanLP = hanlp.load(hanlp.pretrained.mtl.CLOSE_TOK_POS_NER_SRL_DEP_SDP_CON_ELECTRA_BASE_ZH)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bPUHdNJ-sMwW"
      },
      "source": [
        "## 命名实体识别"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wxctCigrTKu-"
      },
      "source": [
        "同时执行所有标准的命名实体识别："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zo08uquCTFSk",
        "outputId": "483a60e4-025b-46aa-866b-284da5808eef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "  \"tok/fine\": [\n",
            "    [\"2021年\", \"HanLPv2.1\", \"为\", \"生产\", \"环境\", \"带来\", \"次\", \"世代\", \"最\", \"先进\", \"的\", \"多\", \"语种\", \"NLP\", \"技术\", \"。\"],\n",
            "    [\"阿婆主\", \"来到\", \"北京\", \"立方庭\", \"参观\", \"自然\", \"语义\", \"科技\", \"公司\", \"。\"]\n",
            "  ],\n",
            "  \"ner/msra\": [\n",
            "    [[\"2021年\", \"DATE\", 0, 1], [\"HanLPv2.1\", \"WWW\", 1, 2]],\n",
            "    [[\"北京\", \"LOCATION\", 2, 3], [\"立方庭\", \"LOCATION\", 3, 4], [\"自然语义科技公司\", \"ORGANIZATION\", 5, 9]]\n",
            "  ],\n",
            "  \"ner/pku\": [\n",
            "    [],\n",
            "    [[\"北京立方庭\", \"ns\", 2, 4], [\"自然语义科技公司\", \"nt\", 5, 9]]\n",
            "  ],\n",
            "  \"ner/ontonotes\": [\n",
            "    [[\"2021年\", \"DATE\", 0, 1], [\"HanLPv2.1\", \"ORG\", 1, 2]],\n",
            "    [[\"北京立方庭\", \"FAC\", 2, 4], [\"自然语义科技公司\", \"ORG\", 5, 9]]\n",
            "  ]\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "print(HanLP(['2021年HanLPv2.1为生产环境带来次世代最先进的多语种NLP技术。', '阿婆主来到北京立方庭参观自然语义科技公司。'], tasks='ner/msra'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6rSNpYzWCKVw"
      },
      "source": [
        "每个四元组表示`[命名实体, 类型标签, 起始下标, 终止下标]`，下标指的是命名实体在单词数组中的下标，单词数组默认为第一个以`tok`开头的数组。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cqEWnj_7p2Lf"
      },
      "source": [
        "任务越少，速度越快。如指定仅执行命名实体识别，默认MSRA标准："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 572
        },
        "id": "BqEmDMGGOtk3",
        "outputId": "33790ca9-7013-456f-c1cb-e5ddce90a457"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Token    \tNER Type        \n",
            "─────────\t────────────────\n",
            "2021年    \t───►DATE        \n",
            "HanLPv2.1\t───►WWW         \n",
            "为        \t                \n",
            "生产       \t                \n",
            "环境       \t                \n",
            "带来       \t                \n",
            "次世代      \t───►DATE        \n",
            "最        \t                \n",
            "先进       \t                \n",
            "的        \t                \n",
            "多        \t                \n",
            "语种       \t                \n",
            "NLP      \t                \n",
            "技术       \t                \n",
            "。        \t                \n",
            "阿婆主      \t                \n",
            "来到       \t                \n",
            "北京       \t◄─┐             \n",
            "立方庭      \t◄─┴►ORGANIZATION\n",
            "参观       \t                \n",
            "自然       \t◄─┐             \n",
            "语义       \t  │             \n",
            "科技       \t  ├►ORGANIZATION\n",
            "公司       \t◄─┘             \n",
            "。        \t                \n"
          ]
        }
      ],
      "source": [
        "HanLP('2021年HanLPv2.1为生产环境带来次世代最先进的多语种NLP技术。阿婆主来到北京立方庭参观自然语义科技公司。', tasks='ner').pretty_print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jj1Jk-2sPHYx"
      },
      "source": [
        "执行OntoNotes命名实体识别："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 572
        },
        "id": "1goEC7znPNkI",
        "outputId": "2a97331c-a5fb-4d3c-ccf2-ce2186616c57",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Token    \tNER Type\n",
            "─────────\t────────\n",
            "2021年    \t───►DATE\n",
            "HanLPv2.1\t───►ORG \n",
            "为        \t        \n",
            "生产       \t        \n",
            "环境       \t        \n",
            "带来       \t        \n",
            "次世代      \t        \n",
            "最        \t        \n",
            "先进       \t        \n",
            "的        \t        \n",
            "多        \t        \n",
            "语种       \t        \n",
            "NLP      \t        \n",
            "技术       \t        \n",
            "。        \t        \n",
            "阿婆主      \t        \n",
            "来到       \t        \n",
            "北京       \t◄─┐     \n",
            "立方庭      \t◄─┴►ORG \n",
            "参观       \t        \n",
            "自然       \t◄─┐     \n",
            "语义       \t  │     \n",
            "科技       \t  ├►ORG \n",
            "公司       \t◄─┘     \n",
            "。        \t        \n"
          ]
        }
      ],
      "source": [
        "HanLP('2021年HanLPv2.1为生产环境带来次世代最先进的多语种NLP技术。阿婆主来到北京立方庭参观自然语义科技公司。', tasks='ner/ontonotes').pretty_print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fGy56S_7CKVx"
      },
      "source": [
        "#### 注意\n",
        "Native API的输入单位限定为句子，需使用[多语种分句模型](https://github.com/hankcs/HanLP/blob/master/plugins/hanlp_demo/hanlp_demo/sent_split.py)或[基于规则的分句函数](https://github.com/hankcs/HanLP/blob/master/hanlp/utils/rules.py#L19)先行分句。RESTful同时支持全文、句子、已分词的句子。除此之外，RESTful和native两种API的语义设计完全一致，用户可以无缝互换。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P7CNTDBRsiYa"
      },
      "source": [
        "## 自定义词典"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZXtRTXlBsmtw"
      },
      "source": [
        "自定义词典是NER任务的成员变量，要操作自定义词典，先获取一个NER任务。以MSRA为例："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QgY22h0AszsA"
      },
      "outputs": [],
      "source": [
        "ner = HanLP['ner/msra']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_6fPzuyps98H"
      },
      "source": [
        "### 白名单词典\n",
        "白名单词典中的词语会尽量被输出。当然，HanLP以统计为主，词典的优先级很低。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "id": "plNDyWhws5qg",
        "outputId": "7120d400-022c-42e9-fca9-febe3745d2c9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Token\tNER Type   \n",
            "─────\t───────────\n",
            "2021年\t───►DATE   \n",
            "测试   \t           \n",
            "高血压  \t           \n",
            "是    \t           \n",
            "138  \t───►INTEGER\n",
            "，    \t           \n",
            "时间   \t           \n",
            "是    \t           \n",
            "午饭   \t◄─┐        \n",
            "后    \t◄─┴►TIME   \n",
            "2点45 \t───►TIME   \n",
            "，    \t           \n",
            "低血压  \t           \n",
            "是    \t           \n",
            "44   \t───►INTEGER\n"
          ]
        }
      ],
      "source": [
        "ner.dict_whitelist = {'午饭后': 'TIME'}\n",
        "doc = HanLP('2021年测试高血压是138，时间是午饭后2点45，低血压是44', tasks='ner/msra')\n",
        "doc.pretty_print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aR_8TICmtw_E"
      },
      "source": [
        "### 强制词典\n",
        "如果你读过[《自然语言处理入门》](http://nlp.hankcs.com/book.php)，你就会理解BMESO标注集，于是你可以直接干预统计模型预测的标签，拿到最高优先级的权限。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 268
        },
        "id": "sWPljj3stsEA",
        "outputId": "99c4c281-a5b6-46bb-dffd-c1722fee7aee"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "To\tNER Type    \n",
            "──\t────────────\n",
            "他 \t            \n",
            "在 \t            \n",
            "浙江\t───►LOCATION\n",
            "金华\t───►LOCATION\n",
            "出生\t            \n",
            "， \t            \n",
            "他 \t            \n",
            "的 \t            \n",
            "名字\t            \n",
            "叫 \t            \n",
            "金华\t───►PERSON  \n",
            "。 \t            \n"
          ]
        }
      ],
      "source": [
        "ner.dict_tags = {('名字', '叫', '金华'): ('O', 'O', 'S-PERSON')}\n",
        "HanLP('他在浙江金华出生，他的名字叫金华。', tasks='ner/msra').pretty_print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fkTC0GFxtinZ"
      },
      "source": [
        "### 黑名单词典\n",
        "黑名单中的词语绝对不会被当做命名实体。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 268
        },
        "id": "bIJpgdGauLJK",
        "outputId": "e74ec7ba-00fd-4958-d772-a1d1c40d1033"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "To\tNER Type    \n",
            "──\t────────────\n",
            "他 \t            \n",
            "在 \t            \n",
            "浙江\t───►LOCATION\n",
            "金华\t            \n",
            "出生\t            \n",
            "， \t            \n",
            "他 \t            \n",
            "的 \t            \n",
            "名字\t            \n",
            "叫 \t            \n",
            "金华\t            \n",
            "。 \t            \n"
          ]
        }
      ],
      "source": [
        "ner.dict_blacklist = {'金华'}\n",
        "HanLP('他在浙江金华出生，他的名字叫金华。', tasks='ner/msra').pretty_print()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "ner_mtl.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}